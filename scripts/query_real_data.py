#!/usr/bin/env python3
"""
æŸ¥è¯¢çœŸå®æ•°æ®åº“ä¸­çš„æ•°æ®

é€šè¿‡å®é™…çš„DataAgentå’ŒAPIç«¯ç‚¹æŸ¥è¯¢å·²å­˜å‚¨çš„æ•°æ®
"""
import asyncio
import sys
import os
from datetime import datetime

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

print("ğŸ” æŸ¥è¯¢æ•°æ®åº“ä¸­çš„çœŸå®æ•°æ®")
print("=" * 50)

async def query_recent_news():
    """æŸ¥è¯¢æœ€è¿‘å­˜å‚¨çš„æ–°é—»æ•°æ®"""
    print("\nğŸ“‹ æŸ¥è¯¢æœ€è¿‘å­˜å‚¨çš„æ–°é—»")
    print("-" * 30)
    
    try:
        from src.application.agents.llm_data_agent import LLMDataAgent
        
        # åˆ›å»ºç®€å•çš„æµ‹è¯•repository
        class TestRepo:
            def __init__(self):
                self.news_items = []
                self.vectors = []
            
            async def health_check(self): return True
            
            async def save_news(self, news_data):
                """ä¿å­˜æ–°é—»å¹¶è¿”å›æ ¼å¼åŒ–å¯¹è±¡"""
                news = type('NewsArticle', (), {})()
                news.id = len(self.news_items) + 1
                news.title = news_data.get('title', '')
                news.url = news_data.get('url', '')
                news.content = news_data.get('content', '')
                news.source = news_data.get('source', '')
                news.author = news_data.get('author', '')
                news.content_hash = news_data.get('content_hash', '')
                news.processing_status = 'completed'
                news.published_at = news_data.get('published_at')
                news.created_at = datetime.now()
                news.fetched_at = datetime.now()
                
                self.news_items.append(news)
                return news
            
            async def save_vector(self, vector_data):
                vector = type('VectorEmbedding', (), {})()
                vector.id = len(self.vectors) + 1
                vector.source_type = 'news'
                vector.source_id = str(vector_data.get('source_id', ''))
                vector.embedding = vector_data.get('embedding', [])
                vector.embedding_model = 'text-embedding-ada-002'
                vector.dimension = len(vector_data.get('embedding', []))
                self.vectors.append(vector)
                return vector
                
            async def get_vector_count(self): return len(self.vectors)
            
            async def find_recent_news(self, days=7, limit=None):
                return self.news_items[-limit:] if limit else self.news_items
                
            async def find_recent_analysis(self, days=7, limit=None): return []
        
        repo = TestRepo()
        data_agent = LLMDataAgent(repository=repo)
        
        print("ğŸ”„ æ­¥éª¤1: å…ˆè·å–ä¸€äº›æ–°é—»æ•°æ®...")
        # é¦–å…ˆè·å–ä¸€äº›æ•°æ®
        fetch_result = await data_agent.fetch_news(
            sources=None,
            max_articles=3,
            store_results=True
        )
        
        if fetch_result.success:
            data = fetch_result.result
            print(f"   âœ… æˆåŠŸå­˜å‚¨ {data.get('articles_stored', 0)} ç¯‡æ–‡ç« ")
            print(f"   âœ… åˆ›å»º {data.get('vectors_created', 0)} ä¸ªå‘é‡åµŒå…¥")
            
            # ç°åœ¨æŸ¥è¯¢å­˜å‚¨çš„æ•°æ®
            print(f"\nğŸ”„ æ­¥éª¤2: æŸ¥è¯¢æ•°æ®åº“ä¸­çš„æ•°æ®...")
            
            # ä½¿ç”¨æ•°æ®åº“å·¥å…·æŸ¥è¯¢
            db_tool = None
            for tool in data_agent.tools:
                if 'database' in tool.name.lower() or 'storage' in tool.name.lower():
                    db_tool = tool
                    break
            
            if db_tool:
                query_result = await db_tool.execute(
                    operation="find_recent_news",
                    days=7,
                    limit=10
                )
                
                if query_result.is_success:
                    articles = query_result.data.get('articles', [])
                    count = query_result.data.get('count', 0)
                    
                    print(f"   âœ… æ•°æ®åº“æŸ¥è¯¢æˆåŠŸ: æ‰¾åˆ° {count} ç¯‡æ–‡ç« ")
                    
                    for i, article in enumerate(articles, 1):
                        print(f"\n   ğŸ“° æ–‡ç«  {i}:")
                        print(f"      ID: {article.get('id')}")
                        print(f"      æ ‡é¢˜: {article.get('title', '')[:60]}...")
                        print(f"      æ¥æº: {article.get('source', '')}")
                        print(f"      URL: {article.get('url', '')[:60]}...")
                        print(f"      çŠ¶æ€: {article.get('processing_status', '')}")
                        print(f"      åˆ›å»ºæ—¶é—´: {article.get('created_at', '')}")
                        
                        # æ˜¾ç¤ºå†…å®¹é¢„è§ˆ
                        if 'content' in article and article['content']:
                            content_preview = article['content'][:150] + "..." if len(article['content']) > 150 else article['content']
                            print(f"      å†…å®¹é¢„è§ˆ: {content_preview}")
                        
                    print(f"\nğŸ“Š å­˜å‚¨åº“çŠ¶æ€:")
                    print(f"   ğŸ’¾ æ–°é—»æ–‡ç« : {len(repo.news_items)} ç¯‡")
                    print(f"   ğŸ”— å‘é‡åµŒå…¥: {len(repo.vectors)} ä¸ª")
                    
                    return True
                else:
                    print(f"   âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {query_result.error_message}")
                    return False
            else:
                print("   âŒ æœªæ‰¾åˆ°æ•°æ®åº“å·¥å…·")
                return False
        else:
            print(f"   âŒ æ•°æ®è·å–å¤±è´¥: {fetch_result.error_message}")
            return False
            
    except Exception as e:
        print(f"   âŒ æŸ¥è¯¢è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

async def query_vector_data():
    """æŸ¥è¯¢å‘é‡åµŒå…¥æ•°æ®"""
    print("\nğŸ“‹ æŸ¥è¯¢å‘é‡åµŒå…¥æ•°æ®")
    print("-" * 30)
    
    try:
        from src.application.agents.llm_data_agent import LLMDataAgent
        
        class VectorTestRepo:
            def __init__(self):
                self.vectors = []
                self.news_items = []
                
            async def health_check(self): return True
            async def save_news(self, news): 
                self.news_items.append(news)
                return news
                
            async def save_vector(self, vector_data):
                vector = type('VectorEmbedding', (), {})()
                vector.id = len(self.vectors) + 1
                vector.source_type = 'news'
                vector.source_id = str(vector_data.get('source_id', ''))
                vector.content_hash = vector_data.get('content_hash', '')
                vector.embedding = vector_data.get('embedding', [])
                vector.embedding_model = vector_data.get('embedding_model', 'text-embedding-ada-002')
                vector.dimension = len(vector_data.get('embedding', []))
                vector.created_at = datetime.now()
                self.vectors.append(vector)
                return vector
            
            async def get_vector_count(self): return len(self.vectors)
            async def find_recent_news(self, days=7, limit=None): return []
            async def find_recent_analysis(self, days=7, limit=None): return []
        
        repo = VectorTestRepo()
        data_agent = LLMDataAgent(repository=repo)
        
        print("ğŸ”„ åˆ›å»ºä¸€äº›å‘é‡æ•°æ®...")
        
        # è·å–æ•°æ®å¹¶åˆ›å»ºå‘é‡
        fetch_result = await data_agent.fetch_news(
            sources=None,
            max_articles=2,
            store_results=True
        )
        
        if fetch_result.success and repo.vectors:
            print(f"   âœ… æˆåŠŸåˆ›å»º {len(repo.vectors)} ä¸ªå‘é‡åµŒå…¥")
            
            print(f"\nğŸ”„ æŸ¥è¯¢å‘é‡åµŒå…¥è¯¦æƒ…...")
            
            for i, vector in enumerate(repo.vectors, 1):
                print(f"\n   ğŸ”— å‘é‡ {i}:")
                print(f"      å‘é‡ID: {vector.id}")
                print(f"      æºç±»å‹: {vector.source_type}")
                print(f"      æºID: {vector.source_id}")
                print(f"      å†…å®¹å“ˆå¸Œ: {vector.content_hash}")
                print(f"      åµŒå…¥æ¨¡å‹: {vector.embedding_model}")
                print(f"      å‘é‡ç»´åº¦: {vector.dimension}")
                print(f"      åˆ›å»ºæ—¶é—´: {vector.created_at}")
                
                # æ˜¾ç¤ºå‘é‡çš„å‰5ä¸ªå€¼
                if vector.embedding and len(vector.embedding) > 0:
                    preview = vector.embedding[:5]
                    print(f"      å‘é‡é¢„è§ˆ: [{', '.join(f'{x:.4f}' for x in preview)}...]")
                
                # è®¡ç®—å‘é‡çš„ç»Ÿè®¡ä¿¡æ¯
                if vector.embedding:
                    import statistics
                    mean_val = statistics.mean(vector.embedding)
                    std_val = statistics.stdev(vector.embedding) if len(vector.embedding) > 1 else 0
                    min_val = min(vector.embedding)
                    max_val = max(vector.embedding)
                    
                    print(f"      å‘é‡ç»Ÿè®¡: å‡å€¼={mean_val:.4f}, æ ‡å‡†å·®={std_val:.4f}")
                    print(f"      å€¼èŒƒå›´: [{min_val:.4f}, {max_val:.4f}]")
            
            print(f"\nğŸ“Š å‘é‡å­˜å‚¨æ€»ç»“:")
            print(f"   ğŸ”— æ€»å‘é‡æ•°: {len(repo.vectors)}")
            print(f"   ğŸ“ å‘é‡ç»´åº¦: {repo.vectors[0].dimension if repo.vectors else 'N/A'}")
            print(f"   ğŸ¤– åµŒå…¥æ¨¡å‹: {repo.vectors[0].embedding_model if repo.vectors else 'N/A'}")
            
            return True
        else:
            print(f"   âŒ å‘é‡åˆ›å»ºå¤±è´¥æˆ–æ²¡æœ‰å‘é‡æ•°æ®")
            return False
            
    except Exception as e:
        print(f"   âŒ å‘é‡æŸ¥è¯¢å¼‚å¸¸: {str(e)}")
        return False

async def simulate_api_query():
    """æ¨¡æ‹Ÿé€šè¿‡APIç«¯ç‚¹æŸ¥è¯¢æ•°æ®"""
    print("\nğŸ“‹ æ¨¡æ‹ŸAPIç«¯ç‚¹æŸ¥è¯¢")
    print("-" * 30)
    
    try:
        from src.presentation.api.main_simplified import data_agent
        from src.application.agents.llm_data_agent import LLMDataAgent
        
        # åˆ›å»ºå…¨å±€data_agent (æ¨¡æ‹ŸAPIä¸­çš„åˆå§‹åŒ–)
        class APITestRepo:
            def __init__(self):
                self.stored_articles = []
                self.vectors = []
            
            async def health_check(self): return True
            
            async def save_news(self, news_data):
                article = {
                    'id': len(self.stored_articles) + 1,
                    'title': news_data.get('title', ''),
                    'url': news_data.get('url', ''),
                    'source': news_data.get('source', ''),
                    'content': news_data.get('content', ''),
                    'content_hash': news_data.get('content_hash', ''),
                    'processing_status': 'completed',
                    'created_at': datetime.now(),
                    'published_at': news_data.get('published_at')
                }
                self.stored_articles.append(article)
                return type('Article', (), article)()
            
            async def save_vector(self, vector_data): 
                self.vectors.append(vector_data)
                return vector_data
            async def get_vector_count(self): return len(self.vectors)
            async def find_recent_news(self, days=7, limit=None):
                return self.stored_articles[-limit:] if limit else self.stored_articles
            async def find_recent_analysis(self, days=7, limit=None): return []
        
        # è®¾ç½®å…¨å±€data_agent
        global data_agent
        repo = APITestRepo()
        data_agent = LLMDataAgent(repository=repo)
        
        print("ğŸ”„ æ¨¡æ‹Ÿ /data/recent-news APIè°ƒç”¨...")
        
        # æ¨¡æ‹ŸAPIç«¯ç‚¹çš„é€»è¾‘
        fetch_result = await data_agent.fetch_news(
            sources=None,
            max_articles=3,
            store_results=True
        )
        
        if fetch_result.success:
            result_data = fetch_result.result
            articles_fetched = result_data.get('articles_fetched', 0)
            articles_stored = result_data.get('articles_stored', 0) 
            articles_data = result_data.get('articles', [])
            
            # æ ¼å¼åŒ–APIå“åº”(ç±»ä¼¼çœŸå®API)
            formatted_articles = []
            for article in articles_data:
                formatted_articles.append({
                    "id": article.get('content_hash', 'unknown'),
                    "title": article.get('title', ''),
                    "url": article.get('url', ''),
                    "source": article.get('source', ''),
                    "author": article.get('author', ''),
                    "published_at": article.get('published_at'),
                    "content_preview": article.get('content', '')[:200] + "..." if article.get('content') else "",
                    "processing_status": "completed",
                    "created_at": article.get('fetched_at')
                })
            
            api_response = {
                "success": True,
                "articles_found": len(formatted_articles),
                "articles": formatted_articles,
                "summary": {
                    "articles_fetched": articles_fetched,
                    "articles_stored": articles_stored,
                    "vectors_created": result_data.get('vectors_created', 0),
                    "sources_used": len(result_data.get('sources_used', [])),
                    "execution_time_ms": fetch_result.execution_time_ms
                }
            }
            
            print(f"   âœ… APIå“åº”æˆåŠŸ!")
            print(f"   ğŸ“Š æ‰¾åˆ°æ–‡ç« æ•°: {api_response['articles_found']}")
            print(f"   ğŸ“Š è·å–æ–‡ç« æ•°: {api_response['summary']['articles_fetched']}")
            print(f"   ğŸ“Š å­˜å‚¨æ–‡ç« æ•°: {api_response['summary']['articles_stored']}")
            print(f"   ğŸ“Š å‘é‡åˆ›å»ºæ•°: {api_response['summary']['vectors_created']}")
            print(f"   â±ï¸  æ‰§è¡Œæ—¶é—´: {api_response['summary']['execution_time_ms']}ms")
            
            print(f"\n   ğŸ“° è¿”å›çš„æ–‡ç« :")
            for i, article in enumerate(api_response['articles'], 1):
                print(f"      {i}. {article['title'][:50]}...")
                print(f"         æ¥æº: {article['source']}")
                print(f"         ID: {article['id'][:20]}...")
            
            return True
        else:
            print(f"   âŒ APIè°ƒç”¨å¤±è´¥: {fetch_result.error_message}")
            return False
            
    except Exception as e:
        print(f"   âŒ APIæŸ¥è¯¢å¼‚å¸¸: {str(e)}")
        return False

async def main():
    """è¿è¡Œæ‰€æœ‰æŸ¥è¯¢æµ‹è¯•"""
    print(f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
    print(f"ğŸ”§ OpenAI API: {'âœ… å¯ç”¨' if os.getenv('OPENAI_API_KEY') else 'âŒ ç¼ºå¤±'}")
    
    queries = [
        ("æŸ¥è¯¢æœ€è¿‘æ–°é—»æ•°æ®", query_recent_news),
        ("æŸ¥è¯¢å‘é‡åµŒå…¥æ•°æ®", query_vector_data),
        ("æ¨¡æ‹ŸAPIç«¯ç‚¹æŸ¥è¯¢", simulate_api_query)
    ]
    
    passed = 0
    for query_name, query_func in queries:
        try:
            print(f"\n{'='*50}")
            result = await query_func()
            if result:
                passed += 1
                print(f"\nâœ… {query_name}: æˆåŠŸ")
            else:
                print(f"\nâŒ {query_name}: å¤±è´¥")
        except Exception as e:
            print(f"\nğŸ’¥ {query_name}: å¼‚å¸¸ - {str(e)}")
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ•°æ®åº“æŸ¥è¯¢æµ‹è¯•ç»“æœ: {passed}/{len(queries)} æˆåŠŸ")
    
    if passed == len(queries):
        print("ğŸ‰ æ‰€æœ‰æŸ¥è¯¢æµ‹è¯•é€šè¿‡!")
    else:
        print("âš ï¸  éƒ¨åˆ†æŸ¥è¯¢éœ€è¦æ£€æŸ¥")
    
    print(f"\nğŸ’¡ å®é™…ä½¿ç”¨å»ºè®®:")
    print(f"   ğŸŒ Web API: curl http://localhost:8000/data/recent-news")
    print(f"   ğŸ Python: ä½¿ç”¨ DataAgent.fetch_news() å’Œæ•°æ®åº“å·¥å…·")
    print(f"   ğŸ’¾ SQL: ç›´æ¥æŸ¥è¯¢ PostgreSQL æ•°æ®åº“è¡¨")
    print(f"   ğŸ“Š ç»Ÿè®¡: ä½¿ç”¨ /health ç«¯ç‚¹è·å–ç³»ç»ŸçŠ¶æ€")

if __name__ == "__main__":
    asyncio.run(main())