#!/usr/bin/env python3
"""
æŸ¥è¯¢æ•°æ®åº“ä¸­å·²å­˜å‚¨çš„æ•°æ®

æ¼”ç¤ºå¦‚ä½•é€šè¿‡ä¸åŒæ–¹å¼æŸ¥è¯¢NewsArticleã€AnalysisResultå’ŒVectorEmbeddingæ•°æ®
"""
import asyncio
import sys
import os
from datetime import datetime, timedelta

# Add project root to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

print("ğŸ” æ•°æ®åº“æ•°æ®æŸ¥è¯¢å·¥å…·")
print("=" * 50)

async def query_via_dataagent():
    """æ–¹å¼1: é€šè¿‡DataAgentçš„æ•°æ®åº“å·¥å…·æŸ¥è¯¢"""
    print("\nğŸ“‹ æ–¹å¼1: é€šè¿‡DataAgentæŸ¥è¯¢")
    print("-" * 30)
    
    try:
        from src.application.agents.llm_data_agent import LLMDataAgent
        
        # Mock repository for testing
        class SimpleRepo:
            def __init__(self):
                self.news_items = []
                self.vectors = []
                # æ·»åŠ ä¸€äº›ç¤ºä¾‹æ•°æ®
                for i in range(3):
                    article = type('Article', (), {})()
                    article.id = i + 1
                    article.title = f"Sample Article {i+1}"
                    article.url = f"https://example.com/article{i+1}"
                    article.source = "example.com"
                    article.content_hash = f"hash{i+1}"
                    article.processing_status = "completed"
                    article.created_at = datetime.utcnow()
                    article.published_at = datetime.utcnow() - timedelta(hours=i)
                    self.news_items.append(article)
                    
            async def health_check(self): return True
            async def save_news(self, news): return news
            async def get_vector_count(self): return len(self.vectors)
            async def find_recent_news(self, days=7, limit=None):
                return self.news_items[-limit:] if limit else self.news_items
            async def find_recent_analysis(self, days=7, limit=None): return []
        
        data_agent = LLMDataAgent(repository=SimpleRepo())
        
        # ä½¿ç”¨æ•°æ®åº“å·¥å…·æŸ¥è¯¢æœ€è¿‘çš„æ–°é—»
        db_tool = None
        for tool in data_agent.tools:
            if 'database' in tool.name.lower() or 'storage' in tool.name.lower():
                db_tool = tool
                break
        
        if db_tool:
            # æŸ¥è¯¢æœ€è¿‘7å¤©çš„æ–°é—»
            result = await db_tool.execute(
                operation="find_recent_news",
                days=7,
                limit=10
            )
            
            if result.is_success:
                articles = result.data.get('articles', [])
                count = result.data.get('count', 0)
                
                print(f"   âœ… æŸ¥è¯¢æˆåŠŸ: æ‰¾åˆ° {count} ç¯‡æ–‡ç« ")
                print(f"   ğŸ“Š æŸ¥è¯¢æ¡ä»¶: æœ€è¿‘7å¤©,é™åˆ¶10ç¯‡")
                
                for i, article in enumerate(articles, 1):
                    print(f"\n   ğŸ“° æ–‡ç«  {i}:")
                    print(f"      ID: {article.get('id')}")
                    print(f"      æ ‡é¢˜: {article.get('title', '')[:50]}...")
                    print(f"      æ¥æº: {article.get('source', '')}")
                    print(f"      çŠ¶æ€: {article.get('processing_status', '')}")
                    print(f"      åˆ›å»ºæ—¶é—´: {article.get('created_at', '')}")
            else:
                print(f"   âŒ æŸ¥è¯¢å¤±è´¥: {result.error_message}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ DataAgentæŸ¥è¯¢å¤±è´¥: {str(e)}")
        return False

async def query_via_repository_direct():
    """æ–¹å¼2: ç›´æ¥ä½¿ç”¨RepositoryæŸ¥è¯¢"""
    print("\nğŸ“‹ æ–¹å¼2: ç›´æ¥ä½¿ç”¨RepositoryæŸ¥è¯¢")
    print("-" * 30)
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿçš„repositoryæ¥æ¼”ç¤ºæŸ¥è¯¢æ–¹æ³•
        class MockRepository:
            def __init__(self):
                # æ¨¡æ‹Ÿæ•°æ®åº“ä¸­çš„æ•°æ®
                self.news_data = [
                    {
                        'id': 1,
                        'title': 'Apple Reports Strong Quarterly Earnings',
                        'url': 'https://cnbc.com/apple-earnings',
                        'source': 'cnbc.com',
                        'content': 'Apple Inc. reported better than expected...',
                        'content_hash': 'abc123',
                        'processing_status': 'completed',
                        'created_at': datetime.utcnow(),
                        'published_at': datetime.utcnow() - timedelta(hours=2)
                    },
                    {
                        'id': 2,
                        'title': 'Tesla Stock Surges on New Model Announcement',
                        'url': 'https://marketwatch.com/tesla-surge',
                        'source': 'marketwatch.com',
                        'content': 'Tesla shares jumped 8% after...',
                        'content_hash': 'def456',
                        'processing_status': 'completed',
                        'created_at': datetime.utcnow(),
                        'published_at': datetime.utcnow() - timedelta(hours=4)
                    }
                ]
                
                self.vector_data = [
                    {
                        'id': 1,
                        'source_type': 'news',
                        'source_id': '1',
                        'embedding': [0.1] * 1536,  # æ¨¡æ‹ŸOpenAIåµŒå…¥
                        'embedding_model': 'text-embedding-ada-002',
                        'dimension': 1536,
                        'created_at': datetime.utcnow()
                    }
                ]
            
            async def find_recent_news(self, days=7, limit=10):
                """æŸ¥è¯¢æœ€è¿‘çš„æ–°é—»"""
                cutoff_date = datetime.utcnow() - timedelta(days=days)
                recent_news = [
                    article for article in self.news_data 
                    if article['created_at'] >= cutoff_date
                ]
                return recent_news[:limit] if limit else recent_news
            
            async def find_news_by_id(self, news_id):
                """æ ¹æ®IDæŸ¥è¯¢æ–°é—»"""
                for article in self.news_data:
                    if article['id'] == news_id:
                        return article
                return None
            
            async def find_news_by_source(self, source, limit=10):
                """æ ¹æ®æ¥æºæŸ¥è¯¢æ–°é—»"""
                source_news = [
                    article for article in self.news_data 
                    if source.lower() in article['source'].lower()
                ]
                return source_news[:limit] if limit else source_news
            
            async def find_vectors_by_source_id(self, source_id):
                """æŸ¥è¯¢ç‰¹å®šæ–‡ç« çš„å‘é‡åµŒå…¥"""
                return [
                    vector for vector in self.vector_data 
                    if vector['source_id'] == str(source_id)
                ]
            
            async def get_statistics(self):
                """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
                return {
                    'total_news': len(self.news_data),
                    'total_vectors': len(self.vector_data),
                    'latest_article_time': max(a['created_at'] for a in self.news_data) if self.news_data else None,
                    'sources_count': len(set(a['source'] for a in self.news_data))
                }
        
        repo = MockRepository()
        
        # 1. æŸ¥è¯¢æœ€è¿‘æ–°é—»
        print("ğŸ”„ æŸ¥è¯¢æœ€è¿‘7å¤©çš„æ–°é—»...")
        recent_news = await repo.find_recent_news(days=7, limit=5)
        print(f"   âœ… æ‰¾åˆ° {len(recent_news)} ç¯‡æœ€è¿‘æ–‡ç« ")
        
        for article in recent_news:
            print(f"   ğŸ“° {article['title'][:40]}... (æ¥æº: {article['source']})")
        
        # 2. æ ¹æ®IDæŸ¥è¯¢ç‰¹å®šæ–‡ç« 
        print(f"\nğŸ”„ æ ¹æ®IDæŸ¥è¯¢æ–‡ç« ...")
        article = await repo.find_news_by_id(1)
        if article:
            print(f"   âœ… æ‰¾åˆ°æ–‡ç« :")
            print(f"      æ ‡é¢˜: {article['title']}")
            print(f"      URL: {article['url']}")
            print(f"      å†…å®¹é¢„è§ˆ: {article['content'][:100]}...")
        
        # 3. æ ¹æ®æ¥æºæŸ¥è¯¢
        print(f"\nğŸ”„ æŸ¥è¯¢CNBCæ¥æºçš„æ–‡ç« ...")
        cnbc_articles = await repo.find_news_by_source('cnbc', limit=3)
        print(f"   âœ… æ‰¾åˆ° {len(cnbc_articles)} ç¯‡CNBCæ–‡ç« ")
        
        # 4. æŸ¥è¯¢å‘é‡åµŒå…¥
        print(f"\nğŸ”„ æŸ¥è¯¢æ–‡ç« ID=1çš„å‘é‡åµŒå…¥...")
        vectors = await repo.find_vectors_by_source_id('1')
        if vectors:
            vector = vectors[0]
            print(f"   âœ… æ‰¾åˆ°å‘é‡åµŒå…¥:")
            print(f"      å‘é‡ID: {vector['id']}")
            print(f"      åµŒå…¥æ¨¡å‹: {vector['embedding_model']}")
            print(f"      å‘é‡ç»´åº¦: {vector['dimension']}")
            print(f"      åµŒå…¥é¢„è§ˆ: [{vector['embedding'][:3]}...] (å‰3ä¸ªå€¼)")
        
        # 5. è·å–ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ”„ è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯...")
        stats = await repo.get_statistics()
        print(f"   âœ… æ•°æ®åº“ç»Ÿè®¡:")
        print(f"      æ€»æ–‡ç« æ•°: {stats['total_news']}")
        print(f"      æ€»å‘é‡æ•°: {stats['total_vectors']}")
        print(f"      æ•°æ®æºæ•°: {stats['sources_count']}")
        print(f"      æœ€æ–°æ–‡ç« æ—¶é—´: {stats['latest_article_time']}")
        
        return True
        
    except Exception as e:
        print(f"   âŒ RepositoryæŸ¥è¯¢å¤±è´¥: {str(e)}")
        return False

async def query_via_api_endpoints():
    """æ–¹å¼3: é€šè¿‡APIç«¯ç‚¹æŸ¥è¯¢"""
    print("\nğŸ“‹ æ–¹å¼3: é€šè¿‡APIç«¯ç‚¹æŸ¥è¯¢")
    print("-" * 30)
    
    try:
        print("ğŸ”„ æ¨¡æ‹ŸAPIç«¯ç‚¹æŸ¥è¯¢...")
        
        # æ¨¡æ‹Ÿ /data/recent-news APIè°ƒç”¨
        print("\n   ğŸ“¡ GET /data/recent-news?days=7&limit=5")
        print("   âœ… æ¨¡æ‹Ÿå“åº”:")
        mock_response = {
            "success": True,
            "articles_found": 2,
            "days_requested": 7,
            "limit_requested": 5,
            "articles": [
                {
                    "id": "abc123",
                    "title": "Market Update: Tech Stocks Rally",
                    "url": "https://example.com/tech-rally",
                    "source": "cnbc.com",
                    "processing_status": "completed",
                    "created_at": "2025-08-30T12:00:00Z"
                },
                {
                    "id": "def456",
                    "title": "Federal Reserve Meeting Results",
                    "url": "https://example.com/fed-meeting",
                    "source": "marketwatch.com", 
                    "processing_status": "completed",
                    "created_at": "2025-08-30T11:30:00Z"
                }
            ],
            "summary": {
                "articles_fetched": 2,
                "articles_stored": 2,
                "sources_used": 2
            }
        }
        
        for i, article in enumerate(mock_response["articles"], 1):
            print(f"      ğŸ“° æ–‡ç«  {i}: {article['title']}")
            print(f"         æ¥æº: {article['source']}")
            print(f"         çŠ¶æ€: {article['processing_status']}")
        
        # æ¨¡æ‹Ÿ /data/recent-analysis APIè°ƒç”¨
        print(f"\n   ğŸ“¡ GET /data/recent-analysis?days=7&limit=3")
        print("   âœ… æ¨¡æ‹Ÿå“åº”:")
        mock_analysis = {
            "analyses_found": 2,
            "analyses": [
                {
                    "id": "analysis_1",
                    "article_id": "abc123",
                    "analysis_type": "sentiment",
                    "result": {"sentiment": "positive", "confidence": 0.87},
                    "model_name": "gpt-4o",
                    "created_at": "2025-08-30T12:05:00Z"
                },
                {
                    "id": "analysis_2", 
                    "article_id": "def456",
                    "analysis_type": "topics",
                    "result": {"topics": ["federal_reserve", "interest_rates", "monetary_policy"]},
                    "model_name": "gpt-4o",
                    "created_at": "2025-08-30T11:35:00Z"
                }
            ]
        }
        
        for i, analysis in enumerate(mock_analysis["analyses"], 1):
            print(f"      ğŸ§  åˆ†æ {i}: {analysis['analysis_type']} (æ–‡ç« ID: {analysis['article_id']})")
            print(f"         ç»“æœ: {analysis['result']}")
            print(f"         æ¨¡å‹: {analysis['model_name']}")
        
        print(f"\n   ğŸ’¡ å®é™…ä½¿ç”¨æ–¹æ³•:")
        print(f"      curl http://localhost:8000/data/recent-news?days=7&limit=10")
        print(f"      curl http://localhost:8000/data/recent-analysis?days=3&limit=5")
        
        return True
        
    except Exception as e:
        print(f"   âŒ APIæŸ¥è¯¢æ¼”ç¤ºå¤±è´¥: {str(e)}")
        return False

async def query_with_sql_examples():
    """æ–¹å¼4: SQLæŸ¥è¯¢ç¤ºä¾‹(ä»…å±•ç¤ºæŸ¥è¯¢è¯­å¥)"""
    print("\nğŸ“‹ æ–¹å¼4: ç›´æ¥SQLæŸ¥è¯¢ç¤ºä¾‹")
    print("-" * 30)
    
    print("ğŸ”„ PostgreSQLæŸ¥è¯¢ç¤ºä¾‹...")
    
    sql_examples = [
        {
            "purpose": "æŸ¥è¯¢æœ€è¿‘7å¤©çš„æ–°é—»",
            "sql": """
SELECT id, title, source, url, processing_status, created_at
FROM news_articles 
WHERE created_at >= NOW() - INTERVAL '7 days'
ORDER BY created_at DESC
LIMIT 10;
"""
        },
        {
            "purpose": "æ ¹æ®æ¥æºæŸ¥è¯¢æ–‡ç« ",
            "sql": """
SELECT id, title, url, published_at
FROM news_articles 
WHERE source LIKE '%cnbc%'
ORDER BY published_at DESC;
"""
        },
        {
            "purpose": "æŸ¥è¯¢å¤„ç†çŠ¶æ€ç»Ÿè®¡",
            "sql": """
SELECT processing_status, COUNT(*) as count
FROM news_articles 
GROUP BY processing_status;
"""
        },
        {
            "purpose": "æŸ¥è¯¢æ–‡ç« å’Œå¯¹åº”çš„åˆ†æç»“æœ",
            "sql": """
SELECT 
    n.id,
    n.title,
    n.source,
    a.analysis_type,
    a.confidence_score,
    a.created_at as analysis_time
FROM news_articles n
LEFT JOIN analysis_results a ON n.id = a.article_id
WHERE n.created_at >= NOW() - INTERVAL '3 days'
ORDER BY n.created_at DESC;
"""
        },
        {
            "purpose": "æŸ¥è¯¢å‘é‡åµŒå…¥ä¿¡æ¯",
            "sql": """
SELECT 
    v.id,
    v.source_type,
    v.source_id,
    v.embedding_model,
    v.dimension,
    v.created_at
FROM vector_embeddings v
WHERE v.source_type = 'news'
ORDER BY v.created_at DESC
LIMIT 5;
"""
        },
        {
            "purpose": "æŸ¥è¯¢æ•°æ®åº“ç»Ÿè®¡",
            "sql": """
SELECT 
    'news_articles' as table_name,
    COUNT(*) as total_count,
    MAX(created_at) as latest_entry
FROM news_articles
UNION ALL
SELECT 
    'vector_embeddings' as table_name,
    COUNT(*) as total_count,
    MAX(created_at) as latest_entry  
FROM vector_embeddings
UNION ALL
SELECT 
    'analysis_results' as table_name,
    COUNT(*) as total_count,
    MAX(created_at) as latest_entry
FROM analysis_results;
"""
        }
    ]
    
    for i, example in enumerate(sql_examples, 1):
        print(f"\n   ğŸ“ ç¤ºä¾‹ {i}: {example['purpose']}")
        print(f"   SQL:")
        for line in example['sql'].strip().split('\n'):
            print(f"      {line}")
    
    print(f"\n   ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"      # è¿æ¥åˆ°PostgreSQLæ•°æ®åº“")
    print(f"      psql postgresql://ai_invest:password@localhost:5432/ai_invest")
    print(f"      # æˆ–è€…ä½¿ç”¨Pythonè¿æ¥")
    print(f"      # from sqlalchemy import create_engine, text")
    print(f"      # engine = create_engine(DATABASE_URL)")
    print(f"      # result = engine.execute(text(sql_query))")
    
    return True

async def main():
    """è¿è¡Œæ‰€æœ‰æŸ¥è¯¢ç¤ºä¾‹"""
    print(f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
    
    methods = [
        ("é€šè¿‡DataAgentæŸ¥è¯¢", query_via_dataagent),
        ("ç›´æ¥ä½¿ç”¨Repository", query_via_repository_direct), 
        ("é€šè¿‡APIç«¯ç‚¹æŸ¥è¯¢", query_via_api_endpoints),
        ("SQLæŸ¥è¯¢ç¤ºä¾‹", query_with_sql_examples)
    ]
    
    passed = 0
    for method_name, method_func in methods:
        try:
            result = await method_func()
            if result:
                passed += 1
                print(f"\nâœ… {method_name}: æ¼”ç¤ºæˆåŠŸ")
            else:
                print(f"\nâŒ {method_name}: æ¼”ç¤ºå¤±è´¥")
        except Exception as e:
            print(f"\nğŸ’¥ {method_name}: å¼‚å¸¸ - {str(e)}")
    
    print(f"\n{'='*50}")
    print(f"ğŸ“Š æŸ¥è¯¢æ–¹æ³•æ¼”ç¤ºç»“æœ: {passed}/{len(methods)} æˆåŠŸ")
    
    print(f"\nğŸ’¡ æ€»ç»“ - æ•°æ®åº“æŸ¥è¯¢çš„4ç§æ–¹å¼:")
    print(f"   1. ğŸ¤– DataAgentå·¥å…·: ä½¿ç”¨data_agentçš„æ•°æ®åº“å·¥å…·æŸ¥è¯¢")
    print(f"   2. ğŸ”§ Repositoryç›´æ¥: ä½¿ç”¨repositoryæ¥å£çš„æŸ¥è¯¢æ–¹æ³•") 
    print(f"   3. ğŸ“¡ APIç«¯ç‚¹: é€šè¿‡HTTP APIè·å–æ ¼å¼åŒ–çš„æ•°æ®")
    print(f"   4. ğŸ’¾ SQLç›´æ¥: ä½¿ç”¨åŸç”ŸSQLæŸ¥è¯¢PostgreSQLæ•°æ®åº“")
    
    print(f"\nğŸ¯ æ¨èä½¿ç”¨æ–¹å¼:")
    print(f"   - åº”ç”¨å¼€å‘: ä½¿ç”¨APIç«¯ç‚¹ (/data/recent-news, /data/recent-analysis)")
    print(f"   - æ•°æ®åˆ†æ: ä½¿ç”¨Repositoryæ–¹æ³•æˆ–SQLæŸ¥è¯¢")
    print(f"   - ç³»ç»Ÿç›‘æ§: ä½¿ç”¨DataAgentå·¥å…·è¿›è¡Œå¥åº·æ£€æŸ¥å’Œç»Ÿè®¡")

if __name__ == "__main__":
    asyncio.run(main())